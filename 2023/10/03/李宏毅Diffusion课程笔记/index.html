<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="#Diffusion #DL 扩散模型 - Diffusion Model【李宏毅2023】231001 问题 图片加杂讯到底是什么操作，权重和为1的相加操作？  为什么图片加很多次噪声后的结果可以看作从噪声中直接采样的数据？ 基本概念Denoising Diffusion Probilistic Models(DDPM)    如何运作Reverse Process    第一步：从normal">
<meta property="og:type" content="article">
<meta property="og:title" content="李宏毅Diffusion课程笔记">
<meta property="og:url" content="http://example.com/2023/10/03/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="#Diffusion #DL 扩散模型 - Diffusion Model【李宏毅2023】231001 问题 图片加杂讯到底是什么操作，权重和为1的相加操作？  为什么图片加很多次噪声后的结果可以看作从噪声中直接采样的数据？ 基本概念Denoising Diffusion Probilistic Models(DDPM)    如何运作Reverse Process    第一步：从normal">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/2023-10-03-21-21-53-image.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231001215337.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002173926.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231001215926.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002175127.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002175316.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002175341.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002175514.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002175719.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002180432.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002180546.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002181243.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002181714.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002182021.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002184105.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002185127.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002190032.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002190557.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002191630.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002192709.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002211828.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002213059.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002214019.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002215610.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002215701.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002220116.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002221646.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002222048.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002232148.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002233323.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002233849.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231002234418.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231003001541.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231003001711.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231003002718.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231003002736.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020231003003046.png">
<meta property="article:published_time" content="2023-10-03T13:16:58.000Z">
<meta property="article:modified_time" content="2023-11-05T10:33:45.323Z">
<meta property="article:author" content="Li Mingshuang">
<meta property="article:tag" content="DL">
<meta property="article:tag" content="Diffusion">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/2023-10-03-21-21-53-image.png">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>李宏毅Diffusion课程笔记</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$']]
			}
		  });
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
	
<meta name="generator" content="Hexo 6.2.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="目录"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="目录"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="顶部" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">首页</a></li><!--
     --><!--
       --><li><a href="/about/">关于</a></li><!--
     --><!--
       --><li><a href="/archives/">归档</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="https://github.com/Li-Mingshuang">项目</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="上一篇" href="/2023/10/23/%E7%A0%94%E7%A9%B6%E7%94%9F%E6%B8%85%E5%8D%95/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="下一篇" href="/2023/10/03/Excel%EF%BC%9A%E7%BB%9F%E8%AE%A1%E5%8C%85%E5%90%AB%E7%89%B9%E5%AE%9A%E6%96%87%E6%9C%AC%E7%9A%84%E5%8D%95%E5%85%83%E6%A0%BC%E6%95%B0%E9%87%8F/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="返回顶部" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="分享文章" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2023/10/03/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2023/10/03/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/&text=李宏毅Diffusion课程笔记"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2023/10/03/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/&title=李宏毅Diffusion课程笔记"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2023/10/03/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/&is_video=false&description=李宏毅Diffusion课程笔记"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=李宏毅Diffusion课程笔记&body=Check out this article: http://example.com/2023/10/03/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2023/10/03/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/&title=李宏毅Diffusion课程笔记"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2023/10/03/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/&title=李宏毅Diffusion课程笔记"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2023/10/03/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/&title=李宏毅Diffusion课程笔记"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2023/10/03/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/&title=李宏毅Diffusion课程笔记"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2023/10/03/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/&name=李宏毅Diffusion课程笔记&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2023/10/03/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/&t=李宏毅Diffusion课程笔记"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B-Diffusion-Model%E3%80%90%E6%9D%8E%E5%AE%8F%E6%AF%852023%E3%80%91"><span class="toc-number">1.</span> <span class="toc-text">扩散模型 - Diffusion Model【李宏毅2023】</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98"><span class="toc-number">1.1.</span> <span class="toc-text">问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">1.2.</span> <span class="toc-text">基本概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E8%BF%90%E4%BD%9C"><span class="toc-number">1.2.1.</span> <span class="toc-text">如何运作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Stable-Diffusion"><span class="toc-number">1.3.</span> <span class="toc-text">Stable Diffusion</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%98-1"><span class="toc-number">1.3.1.</span> <span class="toc-text">问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A1%86%E6%9E%B6"><span class="toc-number">1.3.2.</span> <span class="toc-text">框架</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Metric"><span class="toc-number">1.3.3.</span> <span class="toc-text">Metric</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Decoder"><span class="toc-number">1.3.4.</span> <span class="toc-text">Decoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Generation-Model"><span class="toc-number">1.3.5.</span> <span class="toc-text">Generation Model</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86"><span class="toc-number">1.4.</span> <span class="toc-text">数学原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%98-2"><span class="toc-number">1.4.1.</span> <span class="toc-text">问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">1.4.2.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E7%90%86"><span class="toc-number">1.4.3.</span> <span class="toc-text">推理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%9C%AC%E8%B4%A8"><span class="toc-number">1.4.4.</span> <span class="toc-text">图像生成模型的本质</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#VAE-Compute-P-theta-x"><span class="toc-number">1.4.5.</span> <span class="toc-text">VAE: Compute $P_\theta(x)$</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DDPM-Compute-P-theta-x"><span class="toc-number">1.4.6.</span> <span class="toc-text">DDPM: Compute $P_\theta(x)$</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Lower-bound-of-logP-x"><span class="toc-number">1.4.6.1.</span> <span class="toc-text">Lower bound of $logP(x)$</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Maximize-lower-bound"><span class="toc-number">1.4.6.2.</span> <span class="toc-text">Maximize lower bound</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%87%E6%A0%B7%E6%B5%81%E7%A8%8B"><span class="toc-number">1.4.7.</span> <span class="toc-text">采样流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%80%E5%90%8E%E8%BF%98%E8%A6%81%E5%8A%A0%E4%B8%80%E4%B8%AAnoise"><span class="toc-number">1.4.8.</span> <span class="toc-text">为什么最后还要加一个noise</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88Diffusion%E9%82%A3%E4%B9%88%E6%88%90%E5%8A%9F"><span class="toc-number">1.5.</span> <span class="toc-text">为什么Diffusion那么成功</span></a></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        李宏毅Diffusion课程笔记
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">Li Mingshuang</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2023-10-03T13:16:58.000Z" class="dt-published" itemprop="datePublished">2023-10-03</time>
        
      
    </div>


      

      
    <div class="article-tag">
        <i class="fa-solid fa-tag"></i>
        <a class="p-category" href="/tags/DL/" rel="tag">DL</a>, <a class="p-category" href="/tags/Diffusion/" rel="tag">Diffusion</a>
    </div>


    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <p>#Diffusion #DL</p>
<h1 id="扩散模型-Diffusion-Model【李宏毅2023】"><a href="#扩散模型-Diffusion-Model【李宏毅2023】" class="headerlink" title="扩散模型 - Diffusion Model【李宏毅2023】"></a><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV14c411J7f2">扩散模型 - Diffusion Model【李宏毅2023】</a></h1><p>231001</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ol>
<li><p>图片加杂讯到底是什么操作，权重和为1的相加操作？</p>
</li>
<li><p>为什么图片加很多次噪声后的结果可以看作从噪声中直接采样的数据？</p>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>Denoising Diffusion Probilistic Models(DDPM)</p>
<p><img src="/../images/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/2023-10-03-21-21-53-image.png"></p>
</li>
</ol>
<h3 id="如何运作"><a href="#如何运作" class="headerlink" title="如何运作"></a>如何运作</h3><p>Reverse Process<br>    第一步：从normal distribution sample一个vector<br>    denoise的次数事先定好<br>    同一个Denoise model反复使用，多吃一个step输入<br>    <img src="/../images/Pasted%20image%2020231001215337.png" alt="loading-ag-7485"></p>
<p>Denoise Model内部结构<br>    <font color="#d83931">    减法操作神经网络不是很好实现吗？</font><br>    <img src="/../../images/Pasted%20image%2020231002173926.png"></p>
<p>如何训练Noise Predictor<br>    Forward Process（Diffuision Process）<br>    手动给图片加杂讯<br>    每一步加一点噪音，因此就有每一步噪音的GT和加完噪音的结果。</p>
<p>现在一些常见diffusion模型都是使用<a href="%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/LAION.md">LAION</a>数据集<br>    <img src="/../../images/Pasted%20image%2020231001215926.png"></p>
<p>带文字的话，把文字输入加给Denoise Model即可，每一步都是同样文字</p>
<h2 id="Stable-Diffusion"><a href="#Stable-Diffusion" class="headerlink" title="Stable Diffusion"></a>Stable Diffusion</h2><h3 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h3><p><font color="#d83931">Autoregressive生成模型？</font></p>
<p>当前比较好的图像生成模型架构都和SD差不多</p>
<h3 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h3><p>三个module<br>    Text Encoder<br>    Generation Model<br>        需要成对资料<br>    Decoder<br>        不需要文字-图片成对资料（只需要图片）<br>分别训练，然后组合到一起<br><img src="/../../images/Pasted%20image%2020231002175127.png"></p>
<p>常见网络架构<br>    Stable Diffusion<br>        <img src="/../../images/Pasted%20image%2020231002175316.png"><br>    DALL-E series<br>        <img src="/../../images/Pasted%20image%2020231002175341.png"><br>    Imagen<br>        <img src="/../../images/Pasted%20image%2020231002175514.png"></p>
<p>文字encoder对结果影响很大，encoder越大越好<br>    GPT<br>    BERT<br>    T5<br>    <img src="/../../images/Pasted%20image%2020231002175719.png"><br>    FID-10k（sample 10k张）越小越好，CLIP Score越大越好<br>Diffusion中Denoise Model（UNet）大小影响没那么大</p>
<h3 id="Metric"><a href="#Metric" class="headerlink" title="Metric"></a>Metric</h3><ul>
<li>Frechet Inception Distance(FID)<br>  有一个预训练好的图像分类模型，算真实图片和生成图片的分布接近程度<br>  假设两组图像提取的Repr都是高斯分布，算Frechet Distance，越小越好<br>  尽管假设很粗糙，但是效果不错，和人眼评估接近<br>  <img src="/../../images/Pasted%20image%2020231002180432.png"></li>
<li>Contrastive Language-Image Pre-Training(CLIP)<br>  模型中有一个Text Encoder和一个Image Encoder，分别对文字和图像产生一个向量<br>  成对的图片-文字向量越接近越好，不成对的越远越好<br>  <img src="/../../images/Pasted%20image%2020231002180546.png"></li>
</ul>
<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>作用：将中间产物进一步转换为目标图像<br>中间产物：压缩小图&#x2F;Latent Repr<br>如何训练？<br>不需要和文字成对的图片</p>
<ul>
<li><p>中间产物是小图：<br>  Downsample图片，然后训练Decoder upsample图片<br>  eg: Imagen</p>
</li>
<li><p>中间产物是latent repr：<br>  训练一个Auto-encoder，得到repr，再decode<br>  eg: SD, DALLE<br>  <font color="#d83931">但是这个Auto-encoder encode图片得到的repr不一定和Diffusion中产出的一致啊？还是说这里训练出来的encoder，就是最后框架中给图片使用的encoder？</font><br>  <img src="/../../images/Pasted%20image%2020231002181243.png"><br>  Latent Repr可以看作人类看不懂的小图</p>
<h3 id="Generation-Model"><a href="#Generation-Model" class="headerlink" title="Generation Model"></a>Generation Model</h3><p>作用：吃文字Repr，产生中间产物<br>如何训练：</p>
<ol>
<li>图片先经过Encoder得到Repr</li>
<li>Noise加在Latent Repr上，而不是直接加在图片上</li>
<li>然后训一个Noise Predicter<br><img src="/../../images/Pasted%20image%2020231002181714.png"><br>推理过程（生图）：<br><img src="/../../images/Pasted%20image%2020231002182021.png"><br>尽管Midjourney用的是SD，每一个step的产物应该是一个人看不懂的Latent Repr，但是实际可以看到一个模糊的图片逐渐变清晰，原因是Midjourney将每一步的Latent Repr都输给Decoder处理并展示出来。</li>
</ol>
</li>
</ul>
<h2 id="数学原理"><a href="#数学原理" class="headerlink" title="数学原理"></a>数学原理</h2><h3 id="问题-2"><a href="#问题-2" class="headerlink" title="问题"></a>问题</h3><ol>
<li>一直不是很理解深度学习的条件概率</li>
<li></li>
</ol>
<p>VAE vs. Diffusion<br>    <img src="/../../images/Pasted%20image%2020231002184105.png"></p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p><img src="/../../images/Pasted%20image%2020231002185127.png"><br>训练中实际上是直接得到第t步混合噪音的图，而不是概念中每一步一点点加入噪音。</p>
<h3 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h3><p><img src="/../../images/Pasted%20image%2020231002190032.png"></p>
<h3 id="图像生成模型的本质"><a href="#图像生成模型的本质" class="headerlink" title="图像生成模型的本质"></a>图像生成模型的本质</h3><p>共同目标：<br>    从一个简单分布（高斯分布）采样的一个向量，通过一个网络，得到的输出满足一定分布（真实图像+其他约束（如文字）的分布）。<br>实现目标的方式：<br>    采取Maximum Likelihood Estimation使得模型输出的分布和真实图像的分布接近。<br>Maximum Likelihood Estimation：<br>    <img src="/../../images/Pasted%20image%2020231002190557.png"><br>    假设$P_\theta(x^i)$ 可以计算（实际上不可能，因为输出的分布非常复杂）</p>
<p>Maximum Likelihood Estimation和让两个分布接近的关联性<br>    <img src="/../../images/Pasted%20image%2020231002191630.png" alt="|650"><br>    所有生成模型的目标要么是maximum likehood，要么是minimize某一种divergence，比如初始的GAN是minimize JS divergence。</p>
<h3 id="VAE-Compute-P-theta-x"><a href="#VAE-Compute-P-theta-x" class="headerlink" title="VAE: Compute $P_\theta(x)$"></a>VAE: Compute $P_\theta(x)$</h3><p>和Diffusion非常类似，很多VAE中推导过的东西，Diffusion中不用再推导。<br><img src="/../../images/Pasted%20image%2020231002192709.png"><br>这里Mean of Gaussian我认为其实也是一张图片（长度为图片H*W*C的向量），只是把看作生成的多维高斯分布的均值，也就是说生成一个知道均值的高斯分布。</p>
<p><strong>$P_\theta(x|z)&#x3D;1&#x2F;0$ 这个假设合理吗，会不会导致算出来的$P_\theta(x)$不符合概率定义</strong><br>    比如z有10个可能取值，每个概率都是1&#x2F;10，然后有两个真实数据x，对于$x_1$有5个z经过网络可以产生一致的结果，对于$x_2$有6个z经过网络可以产生一致的结果，那么$P_\theta(x_1)&#x3D;5&#x2F;10, P_\theta(x_2)&#x3D;6&#x2F;10$，这样是不是概率和不为1。<br>    哦不对，按照这个定义，对于某个z来说，产生$x_1$或$x_2$是互斥的。 </p>
<p><strong>$P_\theta(x|z)\propto exp(-||G(z)-x||_2)$ 的证明？</strong>  </p>
<p><strong>Lower bound of $logP(x)$</strong><br>    <img src="/../../images/Pasted%20image%2020231002211828.png"><br>    VAE中有办法maximize <em>lower bound</em>，从而期望$P(x)$ 更大。<br>    $q(z|x)$是Encoder，怎么理解?</p>
<h3 id="DDPM-Compute-P-theta-x"><a href="#DDPM-Compute-P-theta-x" class="headerlink" title="DDPM: Compute $P_\theta(x)$"></a>DDPM: Compute $P_\theta(x)$</h3><p> <img src="/../../images/Pasted%20image%2020231002213059.png"><br>    * exp公式中加一个-号；$x_1:x_T$表示从$x_1$到$x_T$。<br>把生成结果看作高斯分布的均值只是一个假设，这个假设简单且效果不错，其他更复杂的假设，或者把方差考虑进来也没有得到更好的结果。<br>    <strong>这里意思是，我们假设的高斯分布方差为1吗？</strong><br>以及中间有些推导，如果不是高斯分布，并不能成立。 </p>
<h4 id="Lower-bound-of-logP-x"><a href="#Lower-bound-of-logP-x" class="headerlink" title="Lower bound of $logP(x)$"></a>Lower bound of $logP(x)$</h4><p>DDPM按VAE中同样的方式也可以推出lower bound<br><img src="/../../images/Pasted%20image%2020231002214019.png"></p>
<h4 id="Maximize-lower-bound"><a href="#Maximize-lower-bound" class="headerlink" title="Maximize lower bound"></a>Maximize lower bound</h4><p>计算$q(x_t|x_{t-1})$</p>
<p>$$<br>\begin{align}<br>x_{t-1}&#x3D;\sqrt{1-\beta_t}<em>x_t+\sqrt\beta_t</em>\epsilon \<br>\epsilon\sim \mathrm N(\mathbf0,I); \beta_1,\beta_2,…,\beta_T<br>\end{align}<br>$$<br>所以$q(x_t|x_{t-1})$还是一个高斯分布<br>    均值是$\sqrt{1-\beta_t}*x_t$<br>    方差是 $\beta_T$</p>
<p>计算$q(x_t|x_{0})$<br>    可以直接被算出来，而不用一步步递推<br>    只用做一次sample<br>    <img src="/../images/Pasted%20image%2020231002215610.png"><br>    <img src="/../images/Pasted%20image%2020231002215701.png"><br>    <img src="/../images/Pasted%20image%2020231002220116.png"></p>
<p>Lower Bound<br>$$E_{q(x_1:x_T|x_0)}[log(\frac{P(x_0:x_T)}{q(x_1:x_T|x_0)})]$$<br>经过一番推导化简成可以算的形式<br>    参考<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2208.11970.pdf">Understanding Diffusion Models: A Unified Perspective</a><br>    <img src="/../../images/Pasted%20image%2020231002221646.png"><br>直接把高斯分布函数代入进去硬推，最后结果还是一个高斯分布<br>    参考<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2208.11970.pdf">Understanding Diffusion Models: A Unified Perspective</a><br>    <img src="/../../images/Pasted%20image%2020231002222048.png"><br>两个高斯分布的KL Divergence有公式解<br>    <img src="/../../images/Pasted%20image%2020231002232148.png"><br>但是其实不需要求解这个KL Divergence，只是要minimize它，怎么minimize？<br>    <img src="/../../images/Pasted%20image%2020231002233323.png"><br>    $q(x_{t-1}|x_t,x_0)$<br>        均值和方差为定值的高斯分布（橙色圆表示），因为Diffusion Process中人工设定好的，和network（Denoise Model）没有关系。<br>    $P(x_{t-1}|x_t)$<br>        由network来决定（橙色圆表示），但是它的方差也是固定住的，不考虑输出分布的variance，即假设输出每个维度的variance都是个定值，有文献试图讨论它，但是没有改善结果。<br>        均值可以改变，由network产生。<br>    让KL Divergence小，即让这两个分布接近。<br>    让这两个分布接近的方式就是，让两个高斯分布的均值接近。</p>
<p>实际上做的是，训练这个Denoise Model，让这两个均值越接近越好。</p>
<h3 id="采样流程"><a href="#采样流程" class="headerlink" title="采样流程"></a>采样流程</h3><ol>
<li><p>从dataset中取一张$x_0$出来，给定$x_0$，sample一个$x_t$<br> <img src="/../../images/Pasted%20image%2020231002233849.png"></p>
</li>
<li><p>Denoise Model输入$x_t$和t，输出的结果要和$q(x_{t-1}|x_t,x_0)$的均值越接近越好。<br> <img src="/../../images/Pasted%20image%2020231002234418.png"><br> 也可以看作是要模型预测$x_{t-1}$，因为$q(x_{t-1}|x_t,x_0)$是$x_{t-1}$的分布，但是实际上$q(x_{t-1}|x_t,x_0)$的均值中不含$x_{t-1}$，只包含$x_{0}$和$x_{t}$。</p>
<p> $q(x_{t-1}|x_t,x_0)$的均值可以化简成：</p>
<pre><code> ![](../../images/Pasted%20image%2020231002231724.png)
</code></pre>
<p> 因此网络真正需要预测的是$\epsilon$，因为别的参数都是前向过程中设定好的，而$x_t$是已知的。</p>
<pre><code> ![](../../images/Pasted%20image%2020231002234913.png)
</code></pre>
<p> 注：$\alpha$是预先设定好的超参，DDPM中也尝试设为可学习参数，但是发现效果并没有很好，所以还是手动设定，至于怎么设定比较好不确定，$\alpha$由$\beta$决定，DDPM中$\beta$是线性增加，别的paper中也提出新的方式。</p>
</li>
</ol>
<h3 id="为什么最后还要加一个noise"><a href="#为什么最后还要加一个noise" class="headerlink" title="为什么最后还要加一个noise"></a>为什么最后还要加一个noise</h3><p>可以解释成：既然说这个Denoise Model的输出是高斯分布，但是实际生成的是Mean of Gaussian，那么sample的时候要加一个noise，代表考虑variance这一项。<br>但是推理的时候为什么不直接取mean呢，因为如果直接取mean代表我们取高斯分布中概率密度最大的输出？（我之前看VAE就有这个问题！！）</p>
<p><strong>没有在文献上找到非常好的答案，以下是lhy的猜测：</strong><br>生成句子时为什么要sample，都是先生成一个概率分布，再从这个概率分布中sample，为什么不直接生成概率最大的结果（文字）？<br>一个好处是，问同一个问题每次会输出不同的答案。<br>为什么一定要这样呢，为什么不让模型固定住每次回答概率最大的句子，有些应用场景也希望这样，为什么需要有随机性？<br>这篇文章也讨论过这个问题：<br>    <a target="_blank" rel="noopener" href="https://openreview.net/forum?id=rygGQyrFvH">The Curious Case of Neural Text Degeneration</a><br>    如果每次取概率最大的词语，机器会一直重复说同样的话。<br>    通过分析发现，人写的句子每一个字也并不是选取概率最大的选择。<br>     <img src="/../../images/Pasted%20image%2020231003001541.png"></p>
<p>在语音上也有类似的观察<br>    Tip：推理时要做dropout才能让模型表现好<br>    按理说只有训练时才加dropout，推理时不加，但是很违反直觉就是这样。<br>    <img src="/../../images/Pasted%20image%2020231003001711.png"></p>
<p>Diffusion Model是一种Autoregressive<br>    一次到位改成N次到位<br>    做Autoregressive时每一步要加一点随机性结果才会好</p>
<p>其他领域的应用<br>    Speech<br>        WaveGrad<br>    Text<br>        应用比较困难，因为文字是离散的<br>        解决方法<br>            把noise加在embedding上<br>                Diffusion-LM<br>                DiffuSeq<br>            改用其他noise</p>
<h2 id="为什么Diffusion那么成功"><a href="#为什么Diffusion那么成功" class="headerlink" title="为什么Diffusion那么成功"></a>为什么Diffusion那么成功</h2><p>这一部分因为忘记Autoregressive概念所以听的不是很懂，明天继续完善一下笔记。以为复杂一直没搜，搜了一下才知道其实很简单，意思就是Seq-Seq模型用过去的信息预测下一个输入。</p>
<p>前面推导不一定那么关键<br>把Autoregressive方法加到Non autoregressive方法中，一步到位改成N步</p>
<p><img src="/../images/Pasted%20image%2020231003002718.png" alt="loading-ag-7274"></p>
<p><img src="/../../images/Pasted%20image%2020231003002736.png"></p>
<p><img src="/../../images/Pasted%20image%2020231003003046.png"></p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">首页</a></li>
        
          <li><a href="/about/">关于</a></li>
        
          <li><a href="/archives/">归档</a></li>
        
          <li><a target="_blank" rel="noopener" href="https://github.com/Li-Mingshuang">项目</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B-Diffusion-Model%E3%80%90%E6%9D%8E%E5%AE%8F%E6%AF%852023%E3%80%91"><span class="toc-number">1.</span> <span class="toc-text">扩散模型 - Diffusion Model【李宏毅2023】</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98"><span class="toc-number">1.1.</span> <span class="toc-text">问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">1.2.</span> <span class="toc-text">基本概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E8%BF%90%E4%BD%9C"><span class="toc-number">1.2.1.</span> <span class="toc-text">如何运作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Stable-Diffusion"><span class="toc-number">1.3.</span> <span class="toc-text">Stable Diffusion</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%98-1"><span class="toc-number">1.3.1.</span> <span class="toc-text">问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A1%86%E6%9E%B6"><span class="toc-number">1.3.2.</span> <span class="toc-text">框架</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Metric"><span class="toc-number">1.3.3.</span> <span class="toc-text">Metric</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Decoder"><span class="toc-number">1.3.4.</span> <span class="toc-text">Decoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Generation-Model"><span class="toc-number">1.3.5.</span> <span class="toc-text">Generation Model</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86"><span class="toc-number">1.4.</span> <span class="toc-text">数学原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%98-2"><span class="toc-number">1.4.1.</span> <span class="toc-text">问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">1.4.2.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E7%90%86"><span class="toc-number">1.4.3.</span> <span class="toc-text">推理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%9C%AC%E8%B4%A8"><span class="toc-number">1.4.4.</span> <span class="toc-text">图像生成模型的本质</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#VAE-Compute-P-theta-x"><span class="toc-number">1.4.5.</span> <span class="toc-text">VAE: Compute $P_\theta(x)$</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DDPM-Compute-P-theta-x"><span class="toc-number">1.4.6.</span> <span class="toc-text">DDPM: Compute $P_\theta(x)$</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Lower-bound-of-logP-x"><span class="toc-number">1.4.6.1.</span> <span class="toc-text">Lower bound of $logP(x)$</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Maximize-lower-bound"><span class="toc-number">1.4.6.2.</span> <span class="toc-text">Maximize lower bound</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%87%E6%A0%B7%E6%B5%81%E7%A8%8B"><span class="toc-number">1.4.7.</span> <span class="toc-text">采样流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%80%E5%90%8E%E8%BF%98%E8%A6%81%E5%8A%A0%E4%B8%80%E4%B8%AAnoise"><span class="toc-number">1.4.8.</span> <span class="toc-text">为什么最后还要加一个noise</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88Diffusion%E9%82%A3%E4%B9%88%E6%88%90%E5%8A%9F"><span class="toc-number">1.5.</span> <span class="toc-text">为什么Diffusion那么成功</span></a></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2023/10/03/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2023/10/03/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/&text=李宏毅Diffusion课程笔记"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2023/10/03/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/&title=李宏毅Diffusion课程笔记"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2023/10/03/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/&is_video=false&description=李宏毅Diffusion课程笔记"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=李宏毅Diffusion课程笔记&body=Check out this article: http://example.com/2023/10/03/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2023/10/03/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/&title=李宏毅Diffusion课程笔记"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2023/10/03/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/&title=李宏毅Diffusion课程笔记"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2023/10/03/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/&title=李宏毅Diffusion课程笔记"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2023/10/03/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/&title=李宏毅Diffusion课程笔记"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2023/10/03/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/&name=李宏毅Diffusion课程笔记&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2023/10/03/%E6%9D%8E%E5%AE%8F%E6%AF%85Diffusion%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/&t=李宏毅Diffusion课程笔记"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2022-2023
    Li Mingshuang
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">首页</a></li><!--
     --><!--
       --><li><a href="/about/">关于</a></li><!--
     --><!--
       --><li><a href="/archives/">归档</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="https://github.com/Li-Mingshuang">项目</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"复制到粘贴板！\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "复制成功！");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
